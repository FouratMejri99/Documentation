## An Analytical Evaluation of Clean Code Quality in AI Generated Code

Due to The rapid advancement of artificial intelligence, particularly large language models, has significantly transformed the software development process. AI-powered tools are now capable of generating functional code across multiple programming languages, assisting developers in tasks ranging from prototyping to full-scale application development. While these systems have demonstrated strong performance in producing syntactically correct and logically valid code, an important challenge remains largely underexplored the quality of AI-generated code in terms of cleanliness, maintainability, and long-term usability.

Clean code is a fundamental concept in software engineering, emphasizing readability, simplicity, consistency, and ease of maintenance. Well-structured code reduces technical debt, improves collaboration among developers, and lowers the cost of future modifications. Traditionally, clean code principles such as meaningful naming, single responsibility, low complexity, and modular design are enforced through human code reviews, style guides, and static analysis tools. However, when code is generated by artificial intelligence, these principles are not always explicitly prioritized, as most AI models are primarily optimized for correctness and task completion rather than code quality.

As AI generated code becomes increasingly integrated into production systems, the lack of analytical frameworks for evaluating its cleanliness poses a significant risk. Code that functions correctly but violates clean code principles can introduce hidden complexity, reduce maintainability, and increase the likelihood of defects over time. This challenge highlights the need for systematic research that goes beyond functional accuracy and examines how well AI-generated code aligns with established software engineering best practices.

This research presents an analytical study of clean code in AI generated software. It aims to evaluate the extent to which artificial intelligence adheres to recognized clean code principles and to identify recurring patterns, strengths, and deficiencies in AI-produced code. By translating qualitative clean code guidelines into measurable metrics such as complexity, modularity, readability, and consistency, this study provides an objective basis for assessing code quality.

Furthermore, this work explores potential strategies for improving clean code generation in AI systems, including prompt constraints, self-refinement mechanisms, and automated feedback loops based on static analysis. The findings of this research seek to contribute to both academic and practical domains by offering insights that can guide the development of cleaner, more maintainable AI assisted software engineering tools.
